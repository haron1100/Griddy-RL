{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from griddy_env import GriddyEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_from_state(state):\n",
    "    key = pickle.dumps(state)\n",
    "    if key not in value_table:\n",
    "        value_table[key]=0 #initialize\n",
    "    return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_value_table(episode_mem, value_table, discount_factor=0.95, alpha=0.5):\n",
    "    all_diffs=[]\n",
    "    for i, mem in reversed(list(enumerate(episode_mem))): #start from terminal state\n",
    "        if i==len(episode_mem)-1: #if terminal state, G=reward\n",
    "            calculated_new_v = episode_mem[i]['reward']\n",
    "        else:\n",
    "            calculated_new_v = mem['reward']+(discount_factor*np.max(greedy_policy(mem['new_observation'], return_action_vals=True)))\n",
    "        key = key_from_state(mem['new_observation'])\n",
    "        diff = abs(value_table[key]-calculated_new_v)\n",
    "        all_diffs.append(diff)\n",
    "        value_table[key] =  value_table[key] + alpha*(calculated_new_v-value_table[key])\n",
    "    return value_table, np.mean(all_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition(state, action):\n",
    "    state = np.copy(state)\n",
    "    agent_pos = list(zip(*np.where(state[2] == 1)))[0]\n",
    "    new_agent_pos = np.array(agent_pos)\n",
    "    if action==0:\n",
    "        new_agent_pos[1]-=1\n",
    "    elif action==1:\n",
    "        new_agent_pos[1]+=1\n",
    "    elif action==2:\n",
    "        new_agent_pos[0]-=1\n",
    "    elif action==3:\n",
    "        new_agent_pos[0]+=1    \n",
    "    new_agent_pos = np.clip(new_agent_pos, 0, 3)\n",
    "\n",
    "    state[2, agent_pos[0], agent_pos[1]] = 0 #moved from this position so it is empty\n",
    "    state[2, new_agent_pos[0], new_agent_pos[1]] = 1 #moved to this position\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(state, return_action_vals=False):\n",
    "    action_values=[]\n",
    "    for test_action in range(4): #for each action\n",
    "        new_state = transition(state, test_action)\n",
    "        key = key_from_state(new_state)\n",
    "        action_values.append(value_table[key])\n",
    "    policy_action = np.argmax(action_values)\n",
    "    if return_action_vals: return action_values\n",
    "    return policy_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, epsilon):\n",
    "    action = env.action_space.sample() if np.random.rand()<epsilon else greedy_policy(state)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(state):\n",
    "    return np.random.randint(0, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_table_viz(value_table):\n",
    "    values = np.zeros((4, 4))\n",
    "    base_st = np.zeros((3, 4, 4), dtype=np.int64)\n",
    "    base_st[0, 3, 3]=1\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            test_st = np.copy(base_st)\n",
    "            test_st[2, i, j] = 1\n",
    "            key = pickle.dumps(test_st)\n",
    "            if key in value_table:\n",
    "                val = value_table[key]\n",
    "            else:\n",
    "                val=0\n",
    "            values[i, j] = val\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_agent(policy, value_table=None, n=5):\n",
    "    try:\n",
    "        for trial_i in range(n):\n",
    "            observation = env.reset()\n",
    "            done=False\n",
    "            t=0\n",
    "            while not done:\n",
    "                if value_table: env.render(value_table_viz(value_table))\n",
    "                else: env.render()\n",
    "                policy_action = policy(observation)\n",
    "                observation, reward, done, info = env.step(policy_action)\n",
    "                time.sleep(0.5)\n",
    "                t+=1\n",
    "            env.render()\n",
    "            time.sleep(1.5)\n",
    "            print(\"Episode {} finished after {} timesteps\".format(trial_i, t))\n",
    "        env.close()\n",
    "    except KeyboardInterrupt:\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GriddyEnv(4, 4)\n",
    "epsilon = 1\n",
    "i_episode=0\n",
    "discount_factor=0.8\n",
    "value_table = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_episodes=100):\n",
    "    global epsilon\n",
    "    global value_table\n",
    "    global i_episode\n",
    "    try:\n",
    "        for _ in range(n_episodes):\n",
    "            observation = env.reset()\n",
    "            episode_mem = []\n",
    "            done=False\n",
    "            t=0\n",
    "            while not done:\n",
    "                env.render()\n",
    "                time.sleep(0.05)\n",
    "                action = epsilon_greedy_policy(observation, epsilon)\n",
    "                new_observation, reward, done, info = env.step(action)\n",
    "                episode_mem.append({'observation':observation,\n",
    "                                    'action':action,\n",
    "                                    'reward':reward,\n",
    "                                    'new_observation':new_observation,\n",
    "                                    'done':done})\n",
    "                observation=new_observation\n",
    "                t+=1\n",
    "                epsilon*=0.999\n",
    "            value_table, v_delta = update_value_table(episode_mem, value_table)\n",
    "            i_episode+=1\n",
    "            print(\"Episode {} finished after {} timesteps. Eplislon={}. V_Delta={}\".format(i_episode, t, epsilon, v_delta))#, end='\\r')\n",
    "            #print(value_table_viz(value_table))\n",
    "            #print()\n",
    "            env.render(value_table_viz(value_table))\n",
    "            time.sleep(2)\n",
    "        env.close()\n",
    "    except KeyboardInterrupt:\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 finished after 149 timesteps. Eplislon=0.8615048875706075. V_Delta=0.05205898401515101\n",
      "Episode 2 finished after 7 timesteps. Eplislon=0.8554924148377159. V_Delta=0.16937896406312042\n",
      "Episode 3 finished after 14 timesteps. Eplislon=0.8435930602951368. V_Delta=0.18900946939319924\n",
      "Episode 4 finished after 16 timesteps. Eplislon=0.8301963316171974. V_Delta=0.17800375121021716\n",
      "Episode 5 finished after 38 timesteps. Eplislon=0.7992255563671304. V_Delta=0.15410292256508373\n",
      "Episode 6 finished after 7 timesteps. Eplislon=0.7936477332643059. V_Delta=0.036209333390592624\n",
      "Episode 7 finished after 3 timesteps. Eplislon=0.7912691702140651. V_Delta=0.03289009094238279\n",
      "Episode 8 finished after 29 timesteps. Eplislon=0.7686407469632577. V_Delta=0.09666403144638631\n",
      "Episode 9 finished after 13 timesteps. Eplislon=0.7587081519483351. V_Delta=0.08409900818192216\n",
      "Episode 10 finished after 18 timesteps. Eplislon=0.7451668707698216. V_Delta=0.03653499062805766\n",
      "Episode 11 finished after 36 timesteps. Eplislon=0.7188050416738131. V_Delta=0.04305485604429834\n",
      "Episode 12 finished after 6 timesteps. Eplislon=0.7145029791340722. V_Delta=0.0067262020278312335\n",
      "Episode 13 finished after 11 timesteps. Eplislon=0.7066826263699144. V_Delta=0.0010908419337106734\n",
      "Episode 14 finished after 12 timesteps. Eplislon=0.698248920785887. V_Delta=0.010327733832233779\n",
      "Episode 15 finished after 3 timesteps. Eplislon=0.6961562680720428. V_Delta=0.00014491202814196727\n",
      "Episode 16 finished after 6 timesteps. Eplislon=0.6919897588949444. V_Delta=0.0007678227602683219\n",
      "Episode 17 finished after 21 timesteps. Eplislon=0.677602375588654. V_Delta=0.006868710040627052\n",
      "Episode 18 finished after 7 timesteps. Eplislon=0.6728733649170395. V_Delta=0.0011679206647712112\n",
      "Episode 19 finished after 2 timesteps. Eplislon=0.6715282910605703. V_Delta=4.0465514757337395e-06\n",
      "Episode 20 finished after 14 timesteps. Eplislon=0.6621877602947683. V_Delta=0.0011002924990233254\n",
      "Episode 21 finished after 6 timesteps. Eplislon=0.6582245533155777. V_Delta=1.891402604144628e-05\n",
      "Episode 22 finished after 5 timesteps. Eplislon=0.654940006215578. V_Delta=0.0001212206586934661\n",
      "Episode 23 finished after 5 timesteps. Eplislon=0.6516718490384363. V_Delta=0.0004429219144523433\n",
      "Episode 24 finished after 22 timesteps. Eplislon=0.6374846057319378. V_Delta=0.0050014203934815845\n",
      "Episode 25 finished after 11 timesteps. Eplislon=0.6305072317473174. V_Delta=0.0001742974348268059\n",
      "Episode 26 finished after 6 timesteps. Eplislon=0.6267336333646188. V_Delta=0.0015527332776685832\n",
      "Episode 27 finished after 17 timesteps. Eplislon=0.6161639726804428. V_Delta=0.00013718411577332164\n",
      "Episode 28 finished after 11 timesteps. Eplislon=0.6094199565354498. V_Delta=0.007114567937878871\n",
      "Episode 29 finished after 2 timesteps. Eplislon=0.6082017260423355. V_Delta=3.416137769829408e-06\n",
      "Episode 30 finished after 14 timesteps. Eplislon=0.5997420274569785. V_Delta=0.010488799789326502\n",
      "Episode 31 finished after 4 timesteps. Eplislon=0.5973466554009469. V_Delta=4.2908410341491177e-07\n",
      "Episode 32 finished after 17 timesteps. Eplislon=0.5872725966265356. V_Delta=5.3359079996869794e-05\n",
      "Episode 33 finished after 7 timesteps. Eplislon=0.5831740006406804. V_Delta=1.1666743701788107e-05\n",
      "Episode 34 finished after 9 timesteps. Eplislon=0.5779463799857277. V_Delta=1.0364837784542718e-05\n",
      "Episode 35 finished after 4 timesteps. Eplislon=0.5756380598328571. V_Delta=4.021757024047501e-06\n",
      "Episode 36 finished after 19 timesteps. Eplislon=0.5647988152354793. V_Delta=0.00041115541820769294\n",
      "Episode 37 finished after 1 timesteps. Eplislon=0.5642340164202437. V_Delta=1.4551915228366852e-11\n",
      "Episode 38 finished after 4 timesteps. Eplislon=0.5619804635022894. V_Delta=6.705726501854414e-09\n",
      "Episode 39 finished after 1 timesteps. Eplislon=0.561418483038787. V_Delta=3.637978807091713e-12\n",
      "Episode 40 finished after 6 timesteps. Eplislon=0.5580583821978482. V_Delta=0.004087142871525821\n",
      "Episode 41 finished after 12 timesteps. Eplislon=0.5513983909676525. V_Delta=0.00020007492667775684\n",
      "Episode 42 finished after 17 timesteps. Eplislon=0.5420992348603912. V_Delta=0.0008256700531130178\n",
      "Episode 43 finished after 2 timesteps. Eplislon=0.5410155784899053. V_Delta=4.1818459806108876e-10\n",
      "Episode 44 finished after 2 timesteps. Eplislon=0.539934088348504. V_Delta=1.881717004437178e-12\n",
      "Episode 45 finished after 5 timesteps. Eplislon=0.5372398118510032. V_Delta=1.7608797309165424e-09\n",
      "Episode 46 finished after 6 timesteps. Eplislon=0.534024420840334. V_Delta=2.1689246784510154e-06\n",
      "Episode 47 finished after 2 timesteps. Eplislon=0.5329569060230741. V_Delta=2.4530377729092834e-13\n",
      "Episode 48 finished after 1 timesteps. Eplislon=0.532423949117051. V_Delta=7.105427357601002e-15\n",
      "Episode 49 finished after 5 timesteps. Eplislon=0.5297671482893791. V_Delta=0.00024355408987377646\n",
      "Episode 50 finished after 4 timesteps. Eplislon=0.5276512561805725. V_Delta=3.436496014697088e-07\n",
      "Episode 51 finished after 2 timesteps. Eplislon=0.5265964813194676. V_Delta=8.326672684688674e-15\n",
      "Episode 52 finished after 20 timesteps. Eplislon=0.5161640072477562. V_Delta=0.0005649460094303338\n",
      "Episode 53 finished after 2 timesteps. Eplislon=0.515132195397268. V_Delta=1.0454820342786775e-10\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_table_viz(value_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 finished after 1 timesteps\n",
      "Episode 1 finished after 3 timesteps\n",
      "Episode 2 finished after 3 timesteps\n",
      "Episode 3 finished after 1 timesteps\n",
      "Episode 4 finished after 3 timesteps\n"
     ]
    }
   ],
   "source": [
    "visualise_agent(greedy_policy, value_table)\n",
    "#v(s t+1) from v(s)\n",
    "#showing a trajectory on gui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- v table visuailsation\n",
    "- breaking it up into small codable chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def calculate_Gs(episode_mem, discount_factor=0.95):\n",
    "    for i, mem in reversed(list(enumerate(episode_mem))): #start from terminal state\n",
    "        if i==len(episode_mem)-1: #if terminal state, G=reward\n",
    "            episode_mem[i]['G']= mem['reward'] \n",
    "        else:\n",
    "            G = mem['reward']+discount_factor*episode_mem[i+1]['G']\n",
    "            episode_mem[i]['G'] = G \n",
    "    return episode_mem\n",
    "def update_value_table(value_table, episode_mem):\n",
    "    all_diffs=[]\n",
    "    for mem in episode_mem:\n",
    "        key = pickle.dumps(mem['new_observation'])\n",
    "        if key not in value_table:\n",
    "            value_table[key]=0 #initialize\n",
    "        new_val = max(value_table[key], mem['G'])\n",
    "        diff = abs(value_table[key]-new_val)\n",
    "        all_diffs.append(diff)\n",
    "        value_table[key] = new_val\n",
    "    return value_table, np.mean(all_diffs)'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
