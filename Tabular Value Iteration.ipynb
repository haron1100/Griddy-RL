{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from griddy_env import GriddyEnvOneHot\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_Gs(episode_mem, discount_factor=0.95):\n",
    "    for i, mem in reversed(list(enumerate(episode_mem))):\n",
    "        if i==len(episode_mem)-1:\n",
    "            episode_mem[i]['G']= mem['reward']\n",
    "        else:\n",
    "            G = mem['reward']+discount_factor*episode_mem[i+1]['G']\n",
    "            episode_mem[i]['G'] = G   \n",
    "    return episode_mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_value_table(value_table, episode_mem, convergence_delta=0.001):\n",
    "    all_diffs=[]\n",
    "    for mem in episode_mem:\n",
    "        key = pickle.dumps(mem['new_observation'])\n",
    "        if key not in value_table:\n",
    "            value_table[key]=0 #initialize\n",
    "        #value_table[key] = max(value_table[key], mem['G'])\n",
    "        new_val = 0.9*value_table[key] + 0.1*mem['G']\n",
    "        diff = abs(value_table[key]-new_val)\n",
    "        all_diffs.append(diff)\n",
    "        value_table[key] = new_val\n",
    "    return value_table, np.mean(all_diffs)<=convergence_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition(state, action):\n",
    "    state = deepcopy(state)\n",
    "    agent_pos = list(zip(*np.where(state[2] == 1)))[0]\n",
    "    new_agent_pos = np.array(agent_pos)\n",
    "    if action==0:\n",
    "        new_agent_pos[1]-=1\n",
    "    elif action==1:\n",
    "        new_agent_pos[1]+=1\n",
    "    elif action==2:\n",
    "        new_agent_pos[0]-=1\n",
    "    elif action==3:\n",
    "        new_agent_pos[0]+=1    \n",
    "    new_agent_pos = np.clip(new_agent_pos, 0, 3)\n",
    "\n",
    "    state[2, agent_pos[0], agent_pos[1]] = 0 #moved from this position so it is empty\n",
    "    state[2, new_agent_pos[0], new_agent_pos[1]] = 1 #moved to this position\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_best_action(state):\n",
    "    action_values=[]\n",
    "    for test_action in range(4): #for each action\n",
    "        new_state = transition(state, test_action)\n",
    "        key = pickle.dumps(new_state)\n",
    "        if key not in value_table: value_table[key] = 0\n",
    "        action_values.append(value_table[key])\n",
    "    policy_action = np.argmax(action_values)\n",
    "    return policy_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_table_viz(value_table):\n",
    "    values = np.zeros((4, 4))\n",
    "    base_st = np.zeros((3, 4, 4), dtype=np.int64)\n",
    "    base_st[0, 3, 3]=1\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            test_st = deepcopy(base_st)\n",
    "            test_st[2, i, j] = 1\n",
    "            #print(test_st)\n",
    "            key = pickle.dumps(test_st)\n",
    "            if key in value_table:\n",
    "                val = value_table[key]\n",
    "            else:\n",
    "                val=0\n",
    "            values[i, j] = val\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GriddyEnvOneHot()\n",
    "epsilon = 1\n",
    "value_table = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\n",
      "Episode finished after 83 timesteps. Eplislon=0.9212341621210596. Converged=False\n",
      "Episode 1\n",
      "Episode finished after 85 timesteps. Eplislon=0.8469758853683546. Converged=False\n",
      "Episode 2\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for i_episode in range(100):\n",
    "        print('Episode', i_episode)\n",
    "        old_observation = env.reset()\n",
    "        done=False\n",
    "        episode_mem = []\n",
    "        t=0\n",
    "        while not done:\n",
    "            env.render()\n",
    "            \n",
    "            policy_action = pick_best_action(old_observation)\n",
    "            action = env.action_space.sample() if np.random.rand()<epsilon else policy_action\n",
    "            #action = env.action_space.sample()\n",
    "            #print(action)\n",
    "            new_observation, reward, done, info = env.step(action)\n",
    "            episode_mem.append({'old_observation':deepcopy(old_observation),\n",
    "                                'action':action,\n",
    "                                'reward':reward,\n",
    "                                'new_observation':deepcopy(new_observation),\n",
    "                                'done':done})\n",
    "            old_observation=deepcopy(new_observation)\n",
    "            t+=1\n",
    "            epsilon*=0.999\n",
    "            #time.sleep(0.5)\n",
    "        env.render()\n",
    "        #time.sleep(0.5)\n",
    "        episode_mem = calculate_Gs(episode_mem)\n",
    "        value_table, converged = update_value_table(value_table, episode_mem)\n",
    "        print(\"Episode finished after {} timesteps. Eplislon={}. Converged={}\".format(t+1, epsilon, converged))\n",
    "    env.close()\n",
    "except KeyboardInterrupt:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.43653329, 0.41237238, 0.58660672, 0.66545277],\n",
       "       [0.40484385, 0.39460289, 0.57862029, 0.70391521],\n",
       "       [0.42031157, 0.50507498, 0.79875324, 0.87779845],\n",
       "       [0.45579892, 0.57541861, 0.74092617, 0.9835768 ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_table_viz(value_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
